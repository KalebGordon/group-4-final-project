{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olhOG5ab1230",
        "outputId": "fa9e7285-058a-45a7-900a-b4b04cf6b39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r                                                                               \rHit:3 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r                                                                               \rHit:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Connecting to security.ub\r                                                                               \rHit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\r                                                                               \rHit:7 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers]\r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:8 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Fetched 222 kB in 1s (201 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "spark_version = 'spark-3.2.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.17.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXv6NhJv16H0",
        "outputId": "ee9d7172-3279-4a33-c623-67a6b82d4ce2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-29 06:32:06--  https://jdbc.postgresql.org/download/postgresql-42.2.17.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1004734 (981K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.17.jar.2’\n",
            "\n",
            "postgresql-42.2.17. 100%[===================>] 981.19K  1.59MB/s    in 0.6s    \n",
            "\n",
            "2023-03-29 06:32:08 (1.59 MB/s) - ‘postgresql-42.2.17.jar.2’ saved [1004734/1004734]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.17.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "EDwK6b9F4FWI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://casey-elizabeth-haley-kaleb-bucket.s3.us-east-2.amazonaws.com/emissions_data.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "emissions_data = spark.read.csv(SparkFiles.get(\"emissions_data.csv\"), sep=\",\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "2KVWbZxO3bOk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url =\"https://casey-elizabeth-haley-kaleb-bucket.s3.us-east-2.amazonaws.com/population_data.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "population_data = spark.read.csv(SparkFiles.get(\"population_data.csv\"), sep=\",\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "oYAZc-yn4MWl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url =\"https://casey-elizabeth-haley-kaleb-bucket.s3.us-east-2.amazonaws.com/death_data.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "death_data = spark.read.csv(SparkFiles.get(\"death_data.csv\"), sep=\",\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "SOcDXkID4NHG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url =\"https://casey-elizabeth-haley-kaleb-bucket.s3.us-east-2.amazonaws.com/emissions_dataframe.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "emissions_dataframe = spark.read.csv(SparkFiles.get(\"emissions_dataframe.csv\"), sep=\",\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "2sdibAUh5MYG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "password = getpass('Enter database password')\n",
        "\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://group-4-database.c0mbmuajokyj.us-east-2.rds.amazonaws.com:5432/postgres\"\n",
        "config = {\"user\":\"postgres\",\n",
        "          \"password\": password,\n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJrjCs_t4adT",
        "outputId": "b2df91ab-4d51-4bf1-8edf-28eb18e7647b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter database password··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emissions_data.write.jdbc(url=jdbc_url, table='emissions_data', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "quIqXJN54wrq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_data.write.jdbc(url=jdbc_url, table='population_data', mode=mode, properties=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-O1iJG2T4xIz",
        "outputId": "4d592b3b-fb22-4d4c-d16e-98a396f8a979"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-63b730505a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'population_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Column \"2000\" not found in schema Some(StructType(StructField(country_name,StringType,true), StructField(y2000,IntegerType,true), StructField(y2001,IntegerType,true), StructField(y2002,IntegerType,true), StructField(y2003,IntegerType,true), StructField(y2004,IntegerType,true), StructField(y2005,IntegerType,true), StructField(y2006,IntegerType,true), StructField(y2007,IntegerType,true), StructField(y2008,IntegerType,true), StructField(y2009,IntegerType,true), StructField(y2010,IntegerType,true), StructField(y2011,IntegerType,true), StructField(y2012,IntegerType,true), StructField(y2013,IntegerType,true), StructField(y2014,IntegerType,true), StructField(y2015,IntegerType,true), StructField(y2016,IntegerType,true), StructField(y2017,IntegerType,true), StructField(y2018,IntegerType,true), StructField(y2019,IntegerType,true), StructField(y2020,IntegerType,true)))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "death_data.write.jdbc(url=jdbc_url, table='death_data', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "lvh577WU4xYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emissions_dataframe.write.jdbc(url=jdbc_url, table='emissions_dataframe', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "kAxORKig5nef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}